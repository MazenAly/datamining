\section*{Exercise 1}
In this assignment, we implement the Random Projection Algorithm for reducing the number of dimensions in the MNIST dataset. The whole source code of the project can be found in the Github repository \url{https://github.com/MazenAly/datamining}.

The idea of random projections is simply if we have points in a vector space that are of a sufficiently high dimension, then these points can be projected into a suitably lower dimensional space in a way which approximately preserves the distances between the points.
We achieve this by randomly generate a $k \times d$ matrix with the assignment stated coefficients.

\begin{lstlisting}[frame=single]
def create_matrix(d, k)    
    mtx_val = 1/sqrt(d)
    prob = 0
    mtx = np.zeros((d, k))
    for i in range(0,d):
        for j in range(0,k):
            random_number = np.random.random_sample()
            if random_number <= 0.5:
                mtx[i,j] = mtx_val
            else:
                mtx[i,j] = -mtx_val        
    return mtx
\end{lstlisting}
    
Then we evaluate how well the Euclidean distance is preserved for the first 20 instances of the dataset by plotting the distortion for all the ${20\choose 2}$ pairs.

We expect the distortion values to be between 0 and 1 (as proved in exercise 2). Besides, when $k$ increases, the number dimensions approaches the original dimension $d$. Thus, the distortion value should approach 1.
   
Our expectations are confirmed by our experiments as in the below figures. 

In figure \ref{k50}, we can see that the most frequent value of distortion values is around 0.25 when $k = 50$.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\textwidth]{shots/k50.png}
\caption{Histogram of the distortion values with k = 50 }
\label{k50}
\end{figure}

In figure \ref{k100}, the most frequent value of distortion values when $k = 100$ is around 0.35.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\textwidth]{shots/k100.png}
\caption{Histogram of the distortion values with k = 100 }
\label{k100}
\end{figure}

As we predict, we can see that the distortion value increases to 0.79 when $k = 500$ as in figure \ref{k500}.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\textwidth]{shots/k500.png}
\caption{Histogram of the distortion values with k = 500 }
\label{k500}
\end{figure} 

In addition, we change our 1-NN implementation of Assignment 1a so that it uses Euclidean Distance instead of Cosine Similarity, as in the below function 

\begin{lstlisting}[frame=single]
def euclidean_distance(v1, v2):
    diff = np.subtract(v1, v2)
    return np.linalg.norm(diff)
\end{lstlisting}

We run our implementation with and without random projection. We scale our vectors after the projection step to partially compensate for the distortion using a scaling factor $ \frac{\sqrt{d}}{\sqrt{k}} $. Note that we run the implementation for 1000 test cases, as it takes a lot of time to experiment the whole 10,000 test cases. 

We perform 4 experiments, with and without Random Projections. When using Random Projections, we experiment 3 times with $k = {50,100,500}$ to compare the results. The 
confusion matrices are shown in table \ref{confusion_matrix}, \ref{confusion_matrix_50}, \ref{confusion_matrix_100}, and \ref{confusion_matrix_500}.

\subsection*{1-NN without Random Projections}

\begin{lstlisting}[frame=single]
Accuracy of the NN classifier is :  95.57 %
Average Runtime: 0.27 seconds for each instance

Class  0 :
Precision: 0.965517241379
Recall: 0.988235294118

Class  1 :
Precision: 0.9765625
Recall: 0.992063492063

Class  2 :
Precision: 0.982142857143
Recall: 0.948275862069

Class  3 :
Precision: 0.934579439252
Recall: 0.934579439252

Class  4 :
Precision: 0.962962962963
Recall: 0.945454545455

Class  5 :
Precision: 0.923076923077
Recall: 0.965517241379

Class  6 :
Precision: 1.0
Recall: 0.977011494253

Class  7 :
Precision: 0.920792079208
Recall: 0.939393939394

Class  8 :
Precision: 0.97619047619
Recall: 0.921348314607

Class  9 :
Precision: 0.927835051546
Recall: 0.957446808511
\end{lstlisting}

\begin{table}[]
  \centering
\caption{The final confusion matrix without RF}
\label{confusion_matrix}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Predicted-TrueLabel} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{Total} \\ \hline
\textbf{0}                   & 84         & 0          & 1          & 0          & 0          & 0          & 1          & 0          & 1          & 0          & 87             \\ \hline
\textbf{1}                   & 0          & 125        & 0          & 0          & 1          & 0          & 0          & 2          & 0          & 0          & 128            \\ \hline
\textbf{2}                   & 0          & 1          & 110        & 1          & 0          & 0          & 0          & 0          & 0          & 0          & 112            \\ \hline
\textbf{3}                   & 0          & 0          & 0          & 100        & 0          & 2          & 0          & 2          & 2          & 1          & 107            \\ \hline
\textbf{4}                   & 0          & 0          & 1          & 0          & 104        & 1          & 1          & 1          & 0          & 0          & 108            \\ \hline
\textbf{5}                   & 1          & 0          & 0          & 3          & 0          & 84         & 0          & 0          & 3          & 0          & 91             \\ \hline
\textbf{6}                   & 0          & 0          & 0          & 0          & 0          & 0          & 85         & 0          & 0          & 0          & 85             \\ \hline
\textbf{7}                   & 0          & 0          & 4          & 2          & 0          & 0          & 0          & 93         & 0          & 2          & 101            \\ \hline
\textbf{8}                   & 0          & 0          & 0          & 1          & 0          & 0          & 0          & 0          & 82         & 1          & 84             \\ \hline
\textbf{9}                   & 0          & 0          & 0          & 0          & 5          & 0          & 0          & 1          & 1          & 90         & 97             \\ \hline
\textbf{Total}               & 85         & 126        & 116        & 107        & 110        & 87         & 87         & 99         & 89         & 94         &                \\ \hline
\end{tabular}
\end{table}

\subsection*{1-NN with Random Projections, $k=50$}

\begin{lstlisting}[frame=single]
Accuracy of the NN classifier is :  92.3 %
Average Runtime: 0.23 seconds for each instance

Class  0 :
Precision: 0.955056179775
Recall: 1.0

Class  1 :
Precision: 0.932835820896
Recall: 0.992063492063

Class  2 :
Precision: 0.951923076923
Recall: 0.853448275862

Class  3 :
Precision: 0.92380952381
Recall: 0.906542056075

Class  4 :
Precision: 0.960784313725
Recall: 0.890909090909

Class  5 :
Precision: 0.873684210526
Recall: 0.954022988506

Class  6 :
Precision: 0.965517241379
Recall: 0.965517241379

Class  7 :
Precision: 0.92
Recall: 0.929292929293

Class  8 :
Precision: 0.890243902439
Recall: 0.820224719101

Class  9 :
Precision: 0.852941176471
Recall: 0.925531914894

\end{lstlisting}

\begin{table}[]
  \centering
\caption{The final confusion matrix with $k = 50$}
\label{confusion_matrix_50}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Predicted-TrueLabel} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{Total} \\ \hline
\textbf{0}                   & 85         & 0          & 1          & 0          & 0          & 0          & 2          & 0          & 1          & 0          & 89             \\ \hline
\textbf{1}                   & 0          & 125        & 2          & 1          & 1          & 0          & 0          & 3          & 1          & 1          & 134            \\ \hline
\textbf{2}                   & 0          & 1          & 99         & 0          & 0          & 0          & 0          & 0          & 4          & 0          & 104            \\ \hline
\textbf{3}                   & 0          & 0          & 2          & 97         & 0          & 1          & 0          & 2          & 1          & 2          & 105            \\ \hline
\textbf{4}                   & 0          & 0          & 0          & 0          & 98         & 0          & 0          & 0          & 1          & 3          & 102            \\ \hline
\textbf{5}                   & 0          & 0          & 1          & 7          & 0          & 83         & 0          & 0          & 4          & 0          & 95             \\ \hline
\textbf{6}                   & 0          & 0          & 2          & 0          & 0          & 0          & 84         & 0          & 1          & 0          & 87             \\ \hline
\textbf{7}                   & 0          & 0          & 2          & 1          & 0          & 2          & 1          & 92         & 1          & 1          & 100            \\ \hline
\textbf{8}                   & 0          & 0          & 7          & 1          & 1          & 0          & 0          & 0          & 73         & 0          & 82             \\ \hline
\textbf{9}                   & 0          & 0          & 0          & 0          & 10         & 1          & 0          & 2          & 2          & 87         & 102            \\ \hline
\textbf{Total}               & 85         & 126        & 116        & 107        & 110        & 87         & 87         & 99         & 89         & 94         &                \\ \hline
\end{tabular}
\end{table}

For 1-NN with Random Projections of $k=100$

\begin{lstlisting}[frame=single]
Accuracy of the NN classifier is :  94.0 %
Average Runtime: 0.25 seconds for each instance

Class  0 :
Precision: 0.976744186047
Recall: 0.988235294118

Class  1 :
Precision: 0.9765625
Recall: 0.992063492063

Class  2 :
Precision: 0.972972972973
Recall: 0.931034482759

Class  3 :
Precision: 0.924528301887
Recall: 0.915887850467

Class  4 :
Precision: 0.960784313725
Recall: 0.890909090909

Class  5 :
Precision: 0.870967741935
Recall: 0.931034482759

Class  6 :
Precision: 0.965909090909
Recall: 0.977011494253

Class  7 :
Precision: 0.932038834951
Recall: 0.969696969697

Class  8 :
Precision: 0.95
Recall: 0.85393258427

Class  9 :
Precision: 0.864077669903
Recall: 0.946808510638
\end{lstlisting}

\begin{table}[]
  \centering
\caption{The final confusion matrix with RF, $k = 100$}
\label{confusion_matrix_100}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Predicted-TrueLabel} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{Total} \\ \hline
\textbf{0}                   & 84         & 0          & 1          & 0          & 0          & 0          & 1          & 0          & 0          & 0          & 86             \\ \hline
\textbf{1}                   & 0          & 125        & 1          & 0          & 0          & 0          & 0          & 1          & 0          & 1          & 128            \\ \hline
\textbf{2}                   & 0          & 1          & 108        & 1          & 0          & 0          & 0          & 0          & 1          & 0          & 111            \\ \hline
\textbf{3}                   & 0          & 0          & 0          & 98         & 0          & 3          & 0          & 1          & 4          & 0          & 106            \\ \hline
\textbf{4}                   & 0          & 0          & 1          & 0          & 98         & 0          & 1          & 0          & 0          & 2          & 102            \\ \hline
\textbf{5}                   & 0          & 0          & 0          & 3          & 0          & 81         & 0          & 0          & 8          & 1          & 93             \\ \hline
\textbf{6}                   & 1          & 0          & 0          & 0          & 2          & 0          & 85         & 0          & 0          & 0          & 88             \\ \hline
\textbf{7}                   & 0          & 0          & 4          & 2          & 1          & 0          & 0          & 96         & 0          & 0          & 103            \\ \hline
\textbf{8}                   & 0          & 0          & 1          & 1          & 0          & 1          & 0          & 0          & 76         & 1          & 80             \\ \hline
\textbf{9}                   & 0          & 0          & 0          & 2          & 9          & 2          & 0          & 1          & 0          & 89         & 103            \\ \hline
\textbf{Total}               & 85         & 126        & 116        & 107        & 110        & 87         & 87         & 99         & 89         & 94         &                \\ \hline
\end{tabular}
\end{table}

For 1-NN with Random Projections of $k=500$

\begin{lstlisting}[frame=single]
Accuracy of the NN classifier is :  95.6 %
Average Runtime: 0.27 seconds for each instance

Class  0 :
Precision: 0.943820224719
Recall: 0.988235294118

Class  1 :
Precision: 0.984251968504
Recall: 0.992063492063

Class  2 :
Precision: 0.981981981982
Recall: 0.939655172414

Class  3 :
Precision: 0.942857142857
Recall: 0.92523364486

Class  4 :
Precision: 0.963963963964
Recall: 0.972727272727

Class  5 :
Precision: 0.893617021277
Recall: 0.965517241379

Class  6 :
Precision: 0.977011494253
Recall: 0.977011494253

Class  7 :
Precision: 0.949494949495
Recall: 0.949494949495

Class  8 :
Precision: 0.963414634146
Recall: 0.887640449438

Class  9 :
Precision: 0.947368421053
Recall: 0.957446808511
\end{lstlisting}

\begin{table}[]
  \centering
\caption{The final confusion matrix with RF, $k = 500$}
\label{confusion_matrix_500}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Predicted-TrueLabel} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{Total} \\ \hline
\textbf{0}                   & 84         & 0          & 2          & 0          & 0          & 1          & 1          & 0          & 1          & 0          & 89             \\ \hline
\textbf{1}                   & 0          & 125        & 0          & 0          & 0          & 0          & 0          & 2          & 0          & 0          & 127            \\ \hline
\textbf{2}                   & 0          & 1          & 109        & 0          & 0          & 0          & 0          & 0          & 1          & 0          & 111            \\ \hline
\textbf{3}                   & 0          & 0          & 0          & 99         & 0          & 1          & 0          & 2          & 2          & 1          & 105            \\ \hline
\textbf{4}                   & 0          & 0          & 0          & 0          & 107        & 1          & 1          & 1          & 0          & 1          & 111            \\ \hline
\textbf{5}                   & 1          & 0          & 0          & 4          & 0          & 84         & 0          & 0          & 5          & 0          & 94             \\ \hline
\textbf{6}                   & 0          & 0          & 2          & 0          & 0          & 0          & 85         & 0          & 0          & 0          & 87             \\ \hline
\textbf{7}                   & 0          & 0          & 3          & 2          & 0          & 0          & 0          & 94         & 0          & 0          & 99             \\ \hline
\textbf{8}                   & 0          & 0          & 0          & 1          & 0          & 0          & 0          & 0          & 79         & 2          & 82             \\ \hline
\textbf{9}                   & 0          & 0          & 0          & 1          & 3          & 0          & 0          & 0          & 1          & 90         & 95             \\ \hline
\textbf{Total}               & 85         & 126        & 116        & 107        & 110        & 87         & 87         & 99         & 89         & 94         &                \\ \hline
\end{tabular}
\end{table}

We can see that the performance of the classifier with random projections of $k=50$ and $k=100$ is lower than the performance without projections as this huge dimensionality reduction causes a big loss of information. But we can see the performance is improving with the increment of $k$.
For $k=500$, the performance become slightly better than the classifier without projections. The cause could be some noise and outliers were reduced by our dimensionality reduction, and the value of $k$ is close to the original $d$ which preserves most of the information. 

Regarding the runtime, the dimensionality reduction does not show a huge difference among the different settings. It might be because the problem is still not complex enough. If we have thousands of dimensions, then the difference can be clearer. The fastest one is around 0.2 seconds for an instance, and the slowest one (without Random Projections) is around 0.3 seconds for an instance.
